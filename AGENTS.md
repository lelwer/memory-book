# AI-Assisted Development Log (AGENTS.md)

This document details the use of AI agents in the development of the "Memory-to-PDF" Storybook Generator, as required by the final project guidelines.

## 1. AI Tools Used

The following AI tools were used during the development of this project:

- **Gemini (Advanced):** Used as the primary assistant for high-level planning, architectural design, requirements analysis, documentation, and generating boilerplate code.
- **GitHub Copilot (Projected):** Used for real-time, in-editor code completion, primarily for auto-completing repetitive logic, generating docstrings, and writing unit test assertions.

## 2. Process Integration

AI was strategically integrated into every phase of the development lifecycle:

### Phase 1: Planning and Architecture

- **Ideation & Feasibility:** Used Gemini to validate the initial "Memory-to-Storybook" business concept against the final project's technical requirements.
- **Prototyping:** Collaboratively refined the idea into an achievable Python CLI prototype that meets all constraints (testing, CI/CD, etc.) by substituting a website with CLI prompts and book shipping with PDF generation.
- **Scaffolding:** Generated the complete directory structure, `requirements.txt` file, and the initial skeleton code for the project's Python modules.

### Phase 2: Code Generation

- **Language & Standards:** All AI-generated code was required to be in Python 3.10+ and adhere to numpy-style docstrings for all functions and methods.
- **Boilerplate Code:** Generated the initial functions for `src/main.py` (handling user input), `src/api_clients.py` (making API calls), and `src/book_assembler.py` (using the `fpdf2` library).
- **Complex Logic:** Used prompts to understand and implement complex logic, such as how to correctly call the Gemini and image generation APIs, handle authentication, and parse the JSON responses.

### Phase 3: Testing and Debugging

- **Test Case Generation:** Generated `pytest` test functions, particularly for `tests/test_book_assembler.py`.
- **Mocking:** Provided prompts to learn how to use `pytest-mock` to "mock" the live API calls and PDF generation, allowing the CI/CD pipeline to run tests quickly and without API keys.
- **Error Resolution:** Pasted Python tracebacks and error messages directly into the AI to receive explanations and potential solutions.

### Phase 4: Documentation

- **README Generation:** Drafted sections for the `README.md` file, including Installation and Usage instructions.
- **Docstring Completion:** Used GitHub Copilot to auto-complete numpy-style docstring templates for new functions.
- **This File:** This `AGENTS.md` file was generated by Gemini based on our planning conversation.

## 3. Example Prompts

Here are specific examples of prompts that were critical to the project's development:

### Planning & Scaffolding

**Prompt:** *"My idea was to plug into a gemini API and an image generator... So I have to back into this somehow to get a prototype for this project."*

**Result:** This high-level prompt led to the "Memory-to-PDF" CLI prototype plan, identifying the key compromises (CLI prompts, PDF output) needed to satisfy the project requirements.

---

**Prompt:** *"Would you like me to outline the requirements.txt file and the basic Python file structure for this project?"*

**Response:** *"Yes please"*

**Result:** The AI generated the complete file structure and list of dependencies (`google-generativeai`, `fpdf2`, `pytest`, etc.), which established the entire project scaffold.

### Documentation & Standards

**Prompt:** *"Can we create the AGENTS.md file first... The only specifics I would like to see... is that the python language be used and numpy docstrings are used."*

**Result:** This prompt initiated the creation of this document, correctly setting the language and documentation standards that would be enforced by the AI for all future code generation.

### Code Generation (Projected)

**Prompt:** *"Write a Python function for src/api_clients.py called get_story_from_gemini. It should take a string prompt, use the google-generativeai library to get a response, and handle API errors. It must include numpy-style docstrings."*

**Result:** This prompt is designed to generate a complete, well-documented, and error-handled function that adheres to our project's specific coding standards.

## 4. Reflection

### Impact on Development

AI assistance was the key factor in making this ambitious project feasible. It served as a "pair programmer" that accelerated development by:

1. Handling the tedious boilerplate for file I/O and API calls
2. Providing immediate, working examples for complex tasks like PDF generation and test mocking

This allowed me to focus on the creative, high-level logic (the custom prompts) rather than getting stuck on implementation details.

### Challenges and Limitations

- **Specificity is Key:** The AI's output was only useful when given highly specific constraints. Vague prompts led to generic, unhelpful code. The project's success depended on my ability to provide clear "rules" (e.g., "use fpdf2," "use numpy docstrings," "mock the API call").

- **Code Verification:** The AI-generated code was a "first draft," not a final product. I was still responsible for reading, understanding, and debugging it. On several occasions, the AI suggested incorrect parameters for libraries, which required me to check the official documentation to fix.

- **Learning vs. Copying:** It was tempting to copy-paste code without understanding it. I had to consciously use the AI as a teacher, asking "Why does this mock test work?" or "Explain this fpdf2 method" to ensure I was still meeting the project's learning objectives.